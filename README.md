# Tiny Diffusion ‚Äî A Minimal Denoising Diffusion Model

A clean, minimal PyTorch implementation of a **Denoising Diffusion Probabilistic Model (DDPM)**, training on CIFAR-10 to generate 32√ó32 images.  
It‚Äôs compact enough to study in a weekend, but powerful enough to produce compelling results.


---

## Features

- **UNet-style architecture** for noise prediction
- **Cosine Œ≤-schedule** for smoother training
- **Automatic Mixed Precision (AMP)** for faster training
- **Checkpointing** & sample grids during training
- **From-scratch scheduler** (no HuggingFace dependencies)
- **Gradio web demo** to generate images interactively
- Well-structured repo for easy extension (EMA, CFG, conditioning, etc.)

---

## üìÇ Project Structure

```
tiny-diffusion-neural-network/
‚îú‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ config.yaml                # Training hyperparameters
‚îú‚îÄ data/                      # Auto-downloaded CIFAR-10
‚îú‚îÄ models/
‚îÇ  ‚îî‚îÄ unet.py                  # UNet noise predictor
‚îú‚îÄ diffusion/
‚îÇ  ‚îú‚îÄ scheduler.py             # Œ≤-schedule and constants
‚îÇ  ‚îî‚îÄ ddpm.py                   # Forward/reverse processes
‚îú‚îÄ train.py                    # Main training loop
‚îú‚îÄ sample.py                   # Generate from trained model
‚îî‚îÄ app.py                      # Gradio interface
```

---

## Quickstart

### 1) Install dependencies
```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2) Train the model
```bash
python train.py
```
> CIFAR-10 will download automatically. Training takes ~1‚Äì4 hours on a modern GPU.  
> Samples are saved in `runs/default/` every few epochs.

### 3) Generate samples
```bash
python sample.py
```
Outputs `samples.png` with an image grid.

### 4) Launch the web demo
```bash
python app.py
```
Visit the Gradio link to interactively generate images.

---

## ‚öô Configuration

All training parameters are in `config.yaml`:

```yaml
seed: 1337
device: "cuda"
image_size: 32
channels: 3
dataset: "CIFAR10"
batch_size: 128
steps: 1000
beta_schedule: "cosine"
lr: 2.0e-4
epochs: 60
amp: true
samples_per_grid: 64
out_dir: "runs/default"
```

---

## Results

After ~60 epochs on CIFAR-10:

| Epoch | Sample Output |
|-------|---------------|
| 10    | ![](runs/default/samples_e10.png) |
| 30    | ![](runs/default/samples_e30.png) |
| 60    | ![](runs/default/samples_e60.png) |

---



---

**MIT License** ‚Äî feel free to use, modify, and learn from this code.  
Made with ‚ù§Ô∏è for deep learning enthusiasts.
